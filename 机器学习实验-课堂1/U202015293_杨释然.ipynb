{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2caa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ysr2022/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/ysr2022/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    855\n",
      "1     60\n",
      "Name: label, dtype: int64\n",
      "x_train.shape\n",
      "(1368, 409)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       173\n",
      "           1       0.96      0.98      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.97      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       173\n",
      "           1       0.96      0.98      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.97      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       173\n",
      "           1       0.95      1.00      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.98      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       173\n",
      "           1       0.95      1.00      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.98      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       173\n",
      "           1       0.88      0.96      0.92       169\n",
      "\n",
      "    accuracy                           0.92       342\n",
      "   macro avg       0.92      0.92      0.92       342\n",
      "weighted avg       0.92      0.92      0.92       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       173\n",
      "           1       0.95      0.98      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.97      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       173\n",
      "           1       0.96      0.98      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.97      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       173\n",
      "           1       0.95      1.00      0.98       169\n",
      "\n",
      "    accuracy                           0.98       342\n",
      "   macro avg       0.98      0.98      0.98       342\n",
      "weighted avg       0.98      0.98      0.98       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       173\n",
      "           1       0.95      1.00      0.97       169\n",
      "\n",
      "    accuracy                           0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.98      0.97      0.97       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       173\n",
      "           1       0.88      0.96      0.92       169\n",
      "\n",
      "    accuracy                           0.92       342\n",
      "   macro avg       0.92      0.92      0.92       342\n",
      "weighted avg       0.92      0.92      0.92       342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       173\n",
      "           1       0.96      1.00      0.98       169\n",
      "\n",
      "    accuracy                           0.98       342\n",
      "   macro avg       0.98      0.98      0.98       342\n",
      "weighted avg       0.98      0.98      0.98       342\n",
      "\n",
      "0    238\n",
      "1     29\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "\n",
    "# 1. 读取数据\n",
    "def robust_readcsv(path, sep=','):\n",
    "    try:\n",
    "        lines = codecs.open(path).readlines()\n",
    "    except:\n",
    "        lines = codecs.open(path, encoding='latin-1').readlines()\n",
    "    header = lines[0].strip().split(sep)\n",
    "    content = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        try:\n",
    "            index = [i for i, x in enumerate(line) if x == ',']\n",
    "            if len(index) == len(header) - 1:\n",
    "                content.append(line.split(sep))\n",
    "            else:\n",
    "                line_content = []\n",
    "                index = [0] + index\n",
    "                for idx in range(len(header) - 1):\n",
    "                    line_content.append(line[index[idx]:index[idx + 1]].strip(sep))\n",
    "                line_content.append(line[index[len(header) - 1]:].strip(sep))\n",
    "                content.append(line_content)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(content, columns=header)\n",
    "\n",
    "\n",
    "def loadTrainData():\n",
    "    # coding=utf-8\n",
    "\n",
    "    import os\n",
    "\n",
    "    #os.chdir(r'C:\\Projects\\智能家居场景识别挑战赛')\n",
    "    train_cus = pd.read_csv('./dataset/train/cus.csv', sep=',')\n",
    "    test_cus = pd.read_csv('./dataset/test/cus.csv', sep=',')\n",
    "\n",
    "    train_devupdate = robust_readcsv('./dataset/train/devUpdata.csv', sep=',')\n",
    "    test_devupdate = robust_readcsv('./dataset/test/devUpdata.csv', sep=',')\n",
    "\n",
    "    train_control = robust_readcsv('./dataset/train/control.csv', sep=',')\n",
    "    test_control = robust_readcsv('./dataset/test/control.csv', sep=',')\n",
    "\n",
    "    train_devlist = robust_readcsv('./dataset/train/devList.csv', sep=',')\n",
    "    test_devlist = robust_readcsv('./dataset/test/devList.csv', sep=',')\n",
    "\n",
    "    # 数据分析\n",
    "\n",
    "    train_devupdate_feat = train_devupdate.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'data': 'nunique',\n",
    "    })\n",
    "    train_devupdate_feat.reset_index(inplace=True)\n",
    "    train_devupdate_feat.columns = ['uid', 'devupdate_did_count', 'devupdate_data_count']\n",
    "\n",
    "    test_devupdate_feat = test_devupdate.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'data': 'nunique',\n",
    "    })\n",
    "    test_devupdate_feat.reset_index(inplace=True)\n",
    "    test_devupdate_feat.columns = ['uid', 'devupdate_did_count', 'devupdate_data_count']\n",
    "\n",
    "    train_control_feat = train_control.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'form': 'nunique',\n",
    "        'data': 'nunique',\n",
    "    })\n",
    "    train_control_feat.reset_index(inplace=True)\n",
    "    train_control_feat.columns = ['uid', 'devcontrol_did_count',\n",
    "                                  'devcontrol_form_count', 'devcontrol_data_count']\n",
    "\n",
    "    test_control_feat = test_control.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'form': 'nunique',\n",
    "        'data': 'nunique',\n",
    "    })\n",
    "    test_control_feat.reset_index(inplace=True)\n",
    "    test_control_feat.columns = ['uid', 'devcontrol_did_count',\n",
    "                                 'devcontrol_form_count', 'devcontrol_data_count']\n",
    "\n",
    "    train_devlist_feat = train_devlist.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'type': 'nunique',\n",
    "        'area': ['unique', 'nunique', 'count']\n",
    "    })\n",
    "    train_devlist_feat.reset_index(inplace=True)\n",
    "    train_devlist_feat.columns = [x[0] + x[1] for x in train_devlist_feat.columns]\n",
    "\n",
    "    test_devlist_feat = test_devlist.groupby('uid').agg({\n",
    "        'did': 'nunique',\n",
    "        'type': 'nunique',\n",
    "        'area': ['unique', 'nunique', 'count']\n",
    "    })\n",
    "    test_devlist_feat.reset_index(inplace=True)\n",
    "    test_devlist_feat.columns = [x[0] + x[1] for x in test_devlist_feat.columns]\n",
    "\n",
    "    train_feat = train_cus.merge(train_devlist_feat, on='uid')\n",
    "    train_feat = train_feat.merge(train_control_feat, on='uid', how='left')\n",
    "    train_feat = train_feat.merge(train_devupdate_feat, on='uid', how='left')\n",
    "    train_feat.fillna(0, inplace=True)\n",
    "\n",
    "    test_feat = test_cus.merge(test_devlist_feat, on='uid')\n",
    "    test_feat = test_feat.merge(test_control_feat, on='uid', how='left')\n",
    "    test_feat = test_feat.merge(test_devupdate_feat, on='uid', how='left')\n",
    "    test_feat.fillna(0, inplace=True)\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=400)\n",
    "    train_dev_tfidf = tfidf.fit_transform(train_feat['areaunique'].apply(lambda x: ' '.join(x)))\n",
    "    test_dev_tfidf = tfidf.transform(test_feat['areaunique'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    train_dev_tfidf = pd.DataFrame(train_dev_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "    test_dev_tfidf = pd.DataFrame(test_dev_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "    train_feat = pd.concat([train_dev_tfidf, train_feat], axis=1)\n",
    "    x = train_feat.drop(['uid', 'label', 'areaunique'], axis=1)\n",
    "    y = train_feat['label']\n",
    "    test_feat = pd.concat([test_dev_tfidf, test_feat], axis=1)\n",
    "    x_test = test_feat.drop(['uid', 'areaunique'], axis=1)\n",
    "    return x, y, x_test, test_cus\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainData, trainLabel, x_test, test_cus = loadTrainData()\n",
    "    print(trainLabel.value_counts())\n",
    "    # 处理样本不平衡\n",
    "    oversample = SMOTE()\n",
    "    X_train_prepared_smote, y_train_smote = oversample.fit_resample(trainData, trainLabel)\n",
    "\n",
    "    # 划分训练集和测试\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X_train_prepared_smote, y_train_smote, test_size=0.2, shuffle=True,\n",
    "                                                      random_state=0)\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=200, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=200, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5),\n",
    "            RandomForestClassifier(n_estimators=300, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=400, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=300, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=400, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learning_rate=0.1, subsample=0.5, max_depth=6, n_estimators=5),\n",
    "            ]\n",
    "\n",
    "    # 多维数组，用来存放每个模型生成的训练集预测结果\n",
    "    dataset_train = np.zeros((x_train.shape[0], len(clfs)))\n",
    "    print('x_train.shape')\n",
    "    print(x_train.shape)\n",
    "    # 多维数组，用来存放每个模型生成的测试集预测结果\n",
    "    dataset_val = np.zeros((x_val.shape[0], len(clfs)))\n",
    "    dataset_test = np.zeros((x_test.shape[0], len(clfs)))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        # 用五种不一样初级训练集获得五个初级学习器\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_submission = clf.predict(x_val)\n",
    "        print(classification_report(y_val, y_submission))\n",
    "        # 每一个初级学习器得出验证集的预测结果，也就是得出全部训练集的1/5\n",
    "        # 在五次初级学习器循环预测完后就得到全部训练集的预测值\n",
    "        dataset_train[:, j] = clf.predict(x_train)\n",
    "        # 五个初级学习器将test交替训练一遍\n",
    "        dataset_val[:, j] = clf.predict(x_val)\n",
    "        dataset_test[:, j] = clf.predict(x_test)\n",
    "    # 模型融合\n",
    "    clf = GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=19, n_estimators=80)\n",
    "    clf.fit(dataset_train, y_train)  # 此时dataset_train是训练集的预测值\n",
    "    # 生成预测集结果向量\n",
    "    # X[:,1]是numpy中数组的一种写法，表示对一个二维数组，取该二维数组第一维中的所有数据，第二维中取第1个数据\n",
    "    y_submission = clf.predict(dataset_val)\n",
    "    print(classification_report(y_val, y_submission))\n",
    "    test_cus['label'] = clf.predict(dataset_test)\n",
    "    print(test_cus['label'].value_counts())\n",
    "    test_cus.to_csv('2022070502.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffa900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
